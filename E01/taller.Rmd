---
title: "Taller 1 - Regresión Ridge y Lasso"
subtitle: "Análisis Avanzado de Datos"
author: "Geraldine Home Saenz - Jose David Soba Loaiza"
date: "2024-03-06"
output: rmdformats::downcute
bibliography: referencias.bib
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
# Librerías
library(tidyverse)
library(glmnet)
```

# Problema

El conjunto de datos `taller1.txt` contiene la información del perfil genómico de un conjunto de 1200 líneas celulares. Para estas se busca determinar cuáles de los 5000 genes (ubicados en cada columna) son de relevancia para la predicción de la variable respuesta (efectividad del tratamiento anticancer, medida como variable continua). Responda las siguientes preguntas:

```{r, message=FALSE}
# Cargar datos
data <- read_delim("data/taller1.txt", delim = ",")
head(data)
```

## Pregunta 1

¿Hay multicolinealidad en los datos?

Hallar multicolinealidad en los datos implica encontrar correlaciones altas entre las variables. No obstante, al contar con 5000 variables, realizar un análisis visual sobre la matriz de correlaciones es inviable, tanto en la tabla que arroja el comando `cor` como un mapa de calor. Por lo tanto, una buena opción para identificar multicolinealidad es buscar aquellas correlaciones superiores a $|0.5|$ que, de acuerdo con la teoría [@bib01], se consideran "débiles" y a partir de este umbral identificar correlaciones "altas" (superiores a $|0.8|$).

```{r}
# Filtrar las correlaciones superiores a 0.7
data %>% 
  select(-y) %>%
  cor() %>%
  as.data.frame() %>%
  rownames_to_column(var = "var1") %>%
  gather(var2, value, -var1) %>%
  mutate(diagonal = (var1 == var2)) %>%
  filter(diagonal != TRUE,
         abs(value) > 0.5)
```

El anterior bloque de código no arrojó ningún valor, lo cual indica que no hay correlaciones superiores a $|0.5|$ entre las 5000 variables que servirán como predictoras dentro de los ejercicios que se adelantaran más adelante en este taller. Por lo tanto, no hay multicolinealidad en los datos.

## Pregunta 2

Separe aleatoriamente (pero guarde la semilla) su conjunto de datos en dos partes:

- Entrenamiento: 1000 líneas celulares
- Prueba: 200 líneas celulares.

```{r, message=FALSE}
# Semilla
set.seed(123)

# Separar datos
train <- data %>% sample_n(1000)
test <- data %>% anti_join(train)
```

## Pregunta 3

Usando los 1000 datos de entrenamiento, determine los valores de $λr$ y $λl$ de regesión Ridge y Lasso, respectivamente, que minimicen el error cuadrático medio (ECM) mediante validación externa. Utilice el método de validación externa que considere más apropiado.

```{r}
# Variables del modelo - Entrenamiento
X_train <- as.matrix(train[, -1]) # Variables predictoras de entrenamiento
y_train <- train$y # variable respuesta de entrenamiento

# Regresión Ridge
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0, type.measure = "mse", nfolds = 10) # Cross-validation para encontrar el mejor lambda
lambda_ridge <- cv_ridge$lambda.min # Mejor lambda
lambda_ridge

# Regresión Lasso
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, type.measure = "mse", nfolds = 10) # Cross-validation para encontrar el mejor lambda
lambda_lasso <- cv_lasso$lambda.min # Mejor lambda
lambda_lasso
```  

Los valores óptimos de $λr$ y $λl$ para la regresión Ridge y Lasso, respectivamente, que minimizan el error cuadrático medio (ECM) mediante validación externa utilizando los 1000 datos de entrenamiento son `r lambda_ridge` y `r lambda_lasso`. Estos valores fueron seleccionados a través de la técnica de validación cruzada con 10 *folds*.

## Pregunta 4

Ajuste la regresión Ridge y Lasso con los valores estimados de $λr$ y $λl$ obtenidos en (3) usando los 1000 datos de entrenamiento.

```{r}
# Regresión Ridge
modelo_ridge <- glmnet(X_train, y_train, alpha = 0, lambda = lambda_ridge)

# Regresión Lasso
modelo_lasso <- glmnet(X_train, y_train, alpha = 1, lambda = lambda_lasso)
```

## Pregunta 5

Para los modelos ajustados en (4) determine el más apropiado para propósitos de predicción. Considere únicamente el ECM en los 200 datos de prueba para su decisión.

```{r}
# Variables del modelo - Prueba
X_test <- as.matrix(test[, -1]) # Variables predictoras de prueba
y_test <- test$y # variable respuesta de prueba

# Predicciones - Regresión Ridge
pred_ridge <- predict(modelo_ridge, newx = X_test)
mse_ridge <- mean((y_test - pred_ridge)^2)
mse_ridge

# Predicciones - Regresión Lasso
pred_lasso <- predict(modelo_lasso, newx = X_test)
mse_lasso <- mean((y_test - pred_lasso)^2)
mse_lasso
```

Basado en el ECM en los 200 datos de prueba, se determina que el modelo de regresión Lasso (`r mse_lasso`) es más apropiado para propósitos de predicción en comparación con el modelo de regresión Ridge (`r mse_ridge`).

## Pregunta 6

Ajuste el modelo seleccionado en (5) para los 1200 datos. Note que en este punto ya tiene un $λ$ estimado y un modelo seleccionado.

```{r}
# Variables del modelo - Todos los datos
X <- as.matrix(data[, -1]) # Variables predictoras
y <- data$y # Variable respuesta

# Ajuste del modelo seleccionado
modelo <- glmnet(X, y, alpha = 1, lambda = lambda_lasso)
```

## Pregunta 7

Grafique las trazas de los coeficientes en función de la penalización para el modelo ajustado en (6).

```{r}
#lambda_val=c(0,log(seq(2,10,length.out=19)))
modelo <- glmnet(X, y, alpha = 1, lambda = lambda_lasso)
plot(modelo, xvar = "lambda")
```

Al ajustar el modelo a un solo valor de lambda (`r lambda_lasso`) no se puede observar la traza de los coeficientes en función de la penalización. El gráfico de trayectoria de coeficientes es útil cuando se ajusta el modelo para una secuencia de valores de lambda, como en los ejemplos de clase [@bib02] a través de la siguiente línea de código: `lambda_val=c(0,log(seq(2,10,length.out=19)))`.

En cambio, se presenta a continuación un gráfico de barras que muestra los coeficientes para las variables del modelo Lasso (diferentes de 0) en función de su signo:

```{r, fig.width=10, fig.height=10}
# Coeficientes
coefs <- coef(modelo, s=0)
 
# Dataframe
coefs_df <- data.frame(variable = rownames(coefs), coeficiente = as.vector(coefs)) %>%
  mutate(signo = ifelse(coeficiente > 0, "Positivo", "Negativo")) %>%
  filter(coeficiente != 0)

# Gráfico
ggplot(coefs_df, aes(x = coeficiente, y = reorder(variable, coeficiente), fill = signo)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Coeficientes del modelo Lasso",
       x = "Valor del coeficiente",
       y = "Variable") +
  theme(legend.position = "bottom") +
  scale_fill_manual(values = c("Positivo" = "blue", "Negativo" = "red"))
```

## Pregunta 8

En un párrafo resuma los resultados obtenidos dado el objetivo inicial del estudio.

Los resultados de este estudio muestran que los modelos de regresión Ridge y Lasso son herramientas eficaces para predecir la efectividad del tratamiento anticancer en líneas celulares utilizando perfiles genómicos de 5000 genes. Inicialmente, se encontró que no hay evidencias de multicolinealidad entre las variables predictoras, lo que sugiere que los genes seleccionados tienen una baja correlación entre sí. Posteriormente, se dividieron los datos en conjuntos de entrenamiento y prueba, y se seleccionaron los valores óptimos de regularización para los modelos de Ridge y Lasso mediante validación cruzada. Los modelos ajustados fueron evaluados utilizando el error cuadrático medio en los datos de prueba, determinando que el modelo Lasso superó ligeramente al modelo Ridge en términos de capacidad predictiva.

# Referencias
